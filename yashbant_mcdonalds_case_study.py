# -*- coding: utf-8 -*-
"""Yashbant-mcdonalds-case study.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14O4idfGD-qiYqP-dMvASeszHoDr0HeN-

## **1..Exploring Data**
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
!pip install bioinfokit

# Load the data
mcdonalds = pd.read_csv('/content/mcdonalds - mcdonalds.csv')

# Display column names and dimensions
print(mcdonalds.columns)
print(mcdonalds.shape)

# Display the first 3 rows
print(mcdonalds.head(3))

# Convert the data to a matrix and convert "Yes" to 1 and "No" to 0
MD_x = mcdonalds.iloc[:, 0:11].values
MD_x = (MD_x == "Yes").astype(int)

# Calculate column means
col_means = np.round(np.mean(MD_x, axis=0), 2)
print(col_means)

#Gender
labels = ['Female', 'Male']
size = mcdonalds['Gender'].value_counts()
colors = ['pink', 'cyan']
explode = [0, 0.1]
plt.rcParams['figure.figsize'] = (7, 7)
plt.pie(size, colors = colors, explode = explode, labels = labels, shadow = True, autopct = '%.2f%%')
plt.title('Gender', fontsize = 20)
plt.axis('off')
plt.legend()
plt.show()

"""## **PCA from R to Python Text-Book**"""

# Perform PCA
from sklearn.decomposition import PCA
MD_pca = PCA()
MD_pca.fit(MD_x)

# Display PCA summary
print(MD_pca.explained_variance_ratio_)
print(MD_pca.singular_values_)

# Plot PCA projection
projection = MD_pca.transform(MD_x)
plt.scatter(projection[:, 0], projection[:, 1], color='grey')
plt.show()

# Display projection axes
print(MD_pca.components_.T)

from sklearn.preprocessing import LabelEncoder
def labelling(x):
    mcdonalds[x] = LabelEncoder().fit_transform(mcdonalds[x])
    return mcdonalds

cat = ['yummy', 'convenient', 'spicy', 'fattening', 'greasy', 'fast', 'cheap',
       'tasty', 'expensive', 'healthy', 'disgusting']

for i in cat:
    labelling(i)
mcdonalds

x = mcdonalds.loc[:,cat].values
x

"""## **Prinicipal Component Analysis in Python**"""

from sklearn.decomposition import PCA
from sklearn import preprocessing

pca_data = preprocessing.scale(x)

pca = PCA(n_components=11)
pc = pca.fit_transform(x)
names = ['pc1','pc2','pc3','pc4','pc5','pc6','pc7','pc8','pc9','pc10','pc11']
pf = pd.DataFrame(data = pc, columns = names)
pf

#Proportion of Variance (from PC1 to PC11)
pca.explained_variance_ratio_

np.cumsum(pca.explained_variance_ratio_)

from bioinfokit.visuz import cluster
# get PC scores
pca_scores = PCA().fit_transform(x)

# get 2D biplot
cluster.biplot(cscore=pca_scores, loadings=loadings, labels=mcdonalds.columns.values, var1=round(pca.explained_variance_ratio_[0]*100, 2),
    var2=round(pca.explained_variance_ratio_[1]*100, 2),show=True,dim=(10,5))

"""## **Using k-Means**"""

from sklearn.cluster import KMeans
from sklearn.metrics import adjusted_rand_score

# Set seed for reproducibility
np.random.seed(1234)

# Perform PCA
pca = PCA(n_components=2)
projection = pca.fit_transform(MD_x)

# Perform K-means clustering
k_values = range(2, 9)
inertias = []
adjusted_rand_scores = []

for k in k_values:
    kmeans = KMeans(n_clusters=k, random_state=1234)
    labels = kmeans.fit_predict(MD_x)
    inertias.append(kmeans.inertia_)
    adjusted_rand_scores.append(adjusted_rand_score(labels, labels))

# Plot number of segments
plt.plot(k_values, inertias, marker='o')
plt.xlabel("Number of segments")
plt.ylabel("Inertia")
plt.show()

# Plot adjusted Rand index
plt.plot(k_values, adjusted_rand_scores, marker='o')
plt.xlabel("Number of segments")
plt.ylabel("Adjusted Rand index")
plt.show()

# Plot histogram
cluster_labels = KMeans(n_clusters=4, random_state=1234).fit_predict(MD_x)
plt.hist(cluster_labels, bins=range(5), align='left', rwidth=0.8)
plt.xlabel("Segment number")
plt.ylabel("Frequency")
plt.show()

# Plot segment stability
stability_scores = []
for k in range(2, 9):
    kmeans = KMeans(n_clusters=k, random_state=1234)
    labels = kmeans.fit_predict(MD_x)
    stability_scores.append(adjusted_rand_score(labels, cluster_labels))

plt.plot(range(2, 9), stability_scores, marker='o')
plt.xlabel("Segment number")
plt.ylabel("Segment stability")
plt.show()

mcdonalds_eleven = mcdonalds.loc[:,cat]
mcdonalds_eleven

from sklearn.cluster import KMeans
from yellowbrick.cluster import KElbowVisualizer
model = KMeans()
visualizer = KElbowVisualizer(model, k=(1,12)).fit(mcdonalds_eleven)
visualizer.show()

#K-means clustering

kmeans = KMeans(n_clusters=4, init='k-means++', random_state=0).fit(mcdonalds_eleven)
mcdonalds['cluster_num'] = kmeans.labels_ #adding to mcdonalds
print (kmeans.labels_) #Label assigned for each data point
print (kmeans.inertia_) #gives within-cluster sum of squares.
print(kmeans.n_iter_) #number of iterations that k-means algorithm runs to get a minimum within-cluster sum of squares
print(kmeans.cluster_centers_) #Location of the centroids on each cluster.

#To see each cluster size
from collections import Counter
Counter(kmeans.labels_)

#Visulazing clusters
sns.scatterplot(data=pf, x="pc1", y="pc2", hue=kmeans.labels_)
plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1],
            marker="X", c="r", s=80, label="centroids")
plt.legend()
plt.show()

"""## **Using Mixtures of Distributions**"""

import statsmodels.api as sm
# Perform K-means clustering
k_values = range(2, 9)
adjusted_rand_scores = []

for k in k_values:
    kmeans = KMeans(n_clusters=k, random_state=1234)
    kmeans_clusters = kmeans.fit_predict(MD_x)
    adjusted_rand_scores.append(adjusted_rand_score(kmeans_clusters, kmeans_clusters))

# Plot value of information criteria
plt.plot(k_values, adjusted_rand_scores, marker='o')
plt.xlabel("Number of segments")
plt.ylabel("Adjusted Rand index")
plt.show()

# Get model for k=4
kmeans_clusters = KMeans(n_clusters=4, random_state=1234).fit_predict(MD_x)
mixture_clusters = kmeans_clusters

# Cross-tabulate kmeans and mixture cluster assignments
table = pd.crosstab(kmeans_clusters, mixture_clusters, rownames=['kmeans'], colnames=['mixture'])
print(table)

# Create flexmix model with fixed kmeans clusters
X_constant = sm.add_constant(MD_x)
mixture_model = sm.MNLogit(kmeans_clusters, X_constant)
mixture_model_fit = mixture_model.fit()

# Cross-tabulate kmeans and mixture cluster assignments
mixture_clusters_a = mixture_model_fit.predict(X_constant).argmax(axis=1)

table_a = pd.crosstab(kmeans_clusters, mixture_clusters_a, rownames=['kmeans'], colnames=['mixture'])
print(table_a)

# Calculate log-likelihood
loglik_m4a = mixture_model_fit.llf
print("Log Likelihood (MD.m4a):", loglik_m4a)

"""## **2..Describing Segments**"""

from statsmodels.graphics.mosaicplot import mosaic
from itertools import product

crosstab =pd.crosstab(mcdonalds['cluster_num'],mcdonalds['Like'])
#Reordering cols
crosstab = crosstab[['-4','-3','-2','-1','0','+1','+2','+3','+4']]
crosstab

#MOSAIC PLOT
plt.rcParams['figure.figsize'] = (7,5)
mosaic(crosstab.stack())
plt.show()

#Mosaic plot gender vs segment
crosstab_gender =pd.crosstab(mcdonalds['cluster_num'],mcdonalds['Gender'])
crosstab_gender

plt.rcParams['figure.figsize'] = (7,5)
mosaic(crosstab_gender.stack())
plt.show()

#box plot for age

sns.boxplot(x="cluster_num", y="Age", data=mcdonalds)

"""## **3..Selecting (the) Target Segment(s)**"""

#Calculating the mean
#Visit frequency
mcdonalds['VisitFrequency'] = LabelEncoder().fit_transform(mcdonalds['VisitFrequency'])
visit = mcdonalds.groupby('cluster_num')['VisitFrequency'].mean()
visit = visit.to_frame().reset_index()
visit

#Like
mcdonalds['Like'] = LabelEncoder().fit_transform(mcdonalds['Like'])
Like = mcdonalds.groupby('cluster_num')['Like'].mean()
Like = Like.to_frame().reset_index()
Like

#Gender
mcdonalds['Gender'] = LabelEncoder().fit_transform(mcdonalds['Gender'])
Gender = mcdonalds.groupby('cluster_num')['Gender'].mean()
Gender = Gender.to_frame().reset_index()
Gender

segment = Gender.merge(Like, on='cluster_num', how='left').merge(visit, on='cluster_num', how='left')
segment

#Target segments

plt.figure(figsize = (9,4))
sns.scatterplot(x = "VisitFrequency", y = "Like",data=segment,s=400, color="r")
plt.title("Simple segment evaluation plot for the fast food data set",
          fontsize = 15)
plt.xlabel("Visit", fontsize = 12)
plt.ylabel("Like", fontsize = 12)
plt.show()